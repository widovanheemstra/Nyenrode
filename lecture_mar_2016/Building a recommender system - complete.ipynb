{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to create a basic recommender system (RS) for books. The RS will take as input a historic set of book ratings from multiple reviewers. For a given book it then suggests what other books might be interesting. We find this in some online shops as well.\n",
    "\n",
    "The notebook is build up as follows:\n",
    "1. A little background on Recommender Systems\n",
    "2. Data inspection and data preparation\n",
    "3. Building the Recommender System and final exercises\n",
    "\n",
    "This notebook is by no means complete. The purpose is to give a quick hands-on experience with building a data product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Recommender System types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The essence of a RS is to select items an user is most likely interested in. RS exist in many different forms but we can group them into three sets:\n",
    "\n",
    "- Collaborative filtering\n",
    "- Content based filtering\n",
    "- Hybrid forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative Filtering RS uses the 'wisdom of the crowd' principle and are found in two flavors: User Based and Item Based. The input of Collaborative Filtering RS is a user-item matrix with ratings of users for several items. Both RS recommend items by finding similarities between users or items respectively based on the ratings. The assumption is that if you like an item then you probably will like similar items too, or you will like what other people like who share a similar item preference.\n",
    "\n",
    "The key here is similarity.\n",
    "\n",
    "**Item-based** RS are easier to use because the similarities can be calculated in adcance but are of lower quality than User based models. They are mostly found in online systems shops.\n",
    "\n",
    "**User-based** RS are more accurate but do not scale well as the complexity and required computation power increases when more users are used as reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Content based filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content based models focus on user profiles and item descriptions, for its recommendations. These models typically try to recommend items that are similar to what a user has liked in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinations of Collaborative Filtering and Content based filtering can be used to use the best of both worlds and to improve the quality of the RS. It seems that these types of RS are most effective of the three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Inspect data and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going the create a book recommender and for that we need user ratings. We will use a dataset from [Book-Crossing](http://www2.informatik.uni-freiburg.de/~cziegler/BX/) but this one is already transformed into usable user-book-rating records. So the hard work has been done. ;-)\n",
    "\n",
    "We also have to think about how we will evaluate the performance of our RS in the end and therefore how to prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a start we import some python libraries that we require for our prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets read-in the data and show the 5 rows to see how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_books.csv\", sep = \",\", header=None,\n",
    "                         names=['Reviewer', 'Book', 'Rating'])\n",
    "\n",
    "# Let's see how the dataframe looks like\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets get some figures about the data:\n",
    "- What is the size of the data?\n",
    "- How many unique books and unique reviewers?\n",
    "- How sparse is the user-item matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the dimensions of the dataframe\n",
    "dim = data.shape\n",
    "print (\"There are {:d} rows in this dataframe and {:d} features.\".format(dim[0], dim[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_books = pd.unique(data[['Book']].values.ravel()).size\n",
    "print (\"{:d} unique books.\".format(unique_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_reviewers = pd.unique(data[['Reviewer']].values.ravel()).size\n",
    "print (\"{:d} unique reviewers.\".format(unique_reviewers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To see how sparse the matrix is, we calculate the fraction of non-zero cells in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Total matrix would be\n",
    "total_cells = unique_reviewers * unique_books\n",
    "\n",
    "# How sparse is the matrix now\n",
    "perc = dim[0] / float(total_cells) * 100.\n",
    "print ('{:.4f}%'.format(perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Exercise**: How can we evaluate the performance of a top-N recommender system? And how should we handle the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Take out random items (rated) per user and check if the recommender will recommend that item. --> Hit Rate\n",
    "* To be more precise we should take into account the recommended position --> Average Reciprocal Hit Range\n",
    "* Sort groups high to low\n",
    "* Pick top row for test\n",
    "* Pick rest for train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data and store the training and testing data as files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "# This is working code, but will take about 15 minutes to run\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "grouped = data.groupby(['Reviewer'])\n",
    "\n",
    "for k, g in grouped:\n",
    "    # We are only interested in reviewers with more than 2 reviews\n",
    "    if len(g) > 2:\n",
    "        print k\n",
    "        g.sort_values('Rating', ascending=False, inplace=True)\n",
    "        test = test.append(g.iloc[:1])\n",
    "        train = train.append(g.iloc[1:])\n",
    "        \n",
    "train.to_csv('train_books.csv', index=False)\n",
    "test.to_csv('test_books.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train_books.csv\", sep = \",\")\n",
    "test = pd.read_csv(\"test_books.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Train: {:d}, Test: {:d}'.format(train.shape[0], test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Determine the amount of reviews per book and show the top 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top 20 most reviewed books \n",
    "top_books = pd.value_counts(train.Book)\n",
    "top_books.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets show a graph of the top 1000 most reviewed books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_books['row'] = range(1, len(top_books) + 1)\n",
    "top_books[:1000].plot(x=top_books.row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now determine how many reviews each user has given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top Reviewers\n",
    "top_reviewers = pd.value_counts(train.Reviewer)\n",
    "top_reviewers.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question**: Do we see something interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Calculate Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Manhattan-distance** which is the sum of the absolute differences of two vectors.\n",
    "$$d_{u,v} = \\sum_{i=1}^{n} \\left | v_{i} - u_{i} \\right |$$\n",
    "\n",
    "The **Euclidean-distance** which is the most intuitive one and calculated by pythagoras theorem.\n",
    "$$d_{u,v} = \\sqrt {\\sum_{i=1}^{n} (v_{i} - u_{i})^2}$$\n",
    "\n",
    "The **Pearson-correlation** which is a more complex one. It looks at how two variables changes together relatively to their individuals changes. Values range from -1 to 1.\n",
    "$$\\rho_{v,u} = \\frac{\\sum_{i=1}^{n} (v_{i} - \\overline{v})( u_{i} - \\overline{u})}{\\sqrt{\\sum_{i=1}^{n} (v_{i} - \\overline{v})^{2} \\sum_{i=1}^{n}( u_{i} - \\overline{u})^2}}$$\n",
    "\n",
    "The **Cosine-similarity** which measures the distance by calculating the angle from the origin. Values range from -1 to 1.\n",
    "$$\\cos (\\theta) = \\frac{v \\cdot w }{\\left \\| v \\right \\|*\\left \\| w \\right \\|} = \\frac{\\sum_{i=1}^{n} v_{i} u_{i}}{\\sqrt{\\sum_{i=1}^{n} v_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} u_{i}^{2}}}$$\n",
    "\n",
    "\n",
    "For item based RS the cosine-similarity tends to be the best performing method. For user based RS the Pearson-correlation method works best. We will build both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First we start simple by building an item based RS, using Pearson-correlation.\n",
    "\n",
    "**Approach**:\n",
    "- Choose two books\n",
    "- Determine their shared reviewers\n",
    "- Get for each book a list of all reviews of their shared reviewers.\n",
    "- Calculate the Pearson-correlation over these reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 2 books \n",
    "book_1, book_2 = \"Harry Potter and the Chamber of Secrets (Book 2)\", \"Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the shared reviewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting all the reviewers for these books\n",
    "book_1_reviewers = train[train.Book == book_1].Reviewer\n",
    "book_2_reviewers = train[train.Book == book_2].Reviewer\n",
    "\n",
    "# Determine any common reviewers\n",
    "common_reviewers = set(book_1_reviewers).intersection(book_2_reviewers)\n",
    "\n",
    "print \"%d people have reviewed these 2 books\" % len(common_reviewers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all ratings from the shared reviewers per book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking the table with only the common reviewers and get the book name and rating\n",
    "com_rev_book1 = train[(train.Reviewer.isin(common_reviewers)) & (train.Book == book_1)]\n",
    "com_rev_book2 = train[(train.Reviewer.isin(common_reviewers)) & (train.Book == book_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "com_rev_book1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "com_rev_book2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see duplicate reviewers, that must be fixed to calculate the Pearson-correlation as we need two vectors of the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix the duplicates to prevent errors when calculating Pearson-correlation.\n",
    "com_rev_book1.sort_values('Reviewer')\n",
    "com_rev_book1 = com_rev_book1[com_rev_book1.Reviewer.duplicated()==False]\n",
    "\n",
    "com_rev_book1.sort_values('Reviewer')\n",
    "com_rev_book1 = com_rev_book1[com_rev_book1.Reviewer.duplicated()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Pearson-correlation from the Numpy package\n",
    "np.corrcoef(com_rev_book1.Rating, com_rev_book2.Rating)[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two books that look quit correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply this calculation to a bigger set of books we need to define two functions that will help us.\n",
    "\n",
    "1. A function that helps us to retrieve the reviews for a given book while sharing reviewers with another book\n",
    "2. A function that helps us calculating the Pearson-correlation for two sets of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a function that collect the reviews of our common reviewers for a specified book\n",
    "def get_book_reviews(title, common_reviewers):\n",
    "    mask = (train.Reviewer.isin(common_reviewers)) & (train.Book==title)\n",
    "    reviews = train[mask].sort_values('Reviewer')\n",
    "    reviews = reviews[reviews.Reviewer.duplicated()==False]\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_pearson_correlation(book1, book2):\n",
    "    # We start by finding the common reviewers\n",
    "    book_1_reviewers = train[train.Book == book1].Reviewer\n",
    "    book_2_reviewers = train[train.Book == book2].Reviewer\n",
    "    common_reviewers = set(book_1_reviewers).intersection(book_2_reviewers)\n",
    "\n",
    "    # Then we look for the reviews given by common reviewers\n",
    "    book_1_reviews = get_book_reviews(book1, common_reviewers)\n",
    "    book_2_reviews = get_book_reviews(book2, common_reviewers)\n",
    "    \n",
    "    # Calculate the Pearson Correlation Score\n",
    "    return np.corrcoef(book_1_reviews.Rating, book_2_reviews.Rating)[1,0]\n",
    "\n",
    "# Print the correlation score\n",
    "calculate_pearson_correlation(book_1,book_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: build a similar function that calculates the cosine-similarity between two books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosin_sim(v, w):\n",
    "    return np.dot(v, w) / np.math.sqrt(np.dot(v, v) * np.dot(w, w))\n",
    "\n",
    "def calculate_cosine_similarity(book1, book2):\n",
    "    # We start by finding the common reviewers\n",
    "    book_1_reviewers = train[train.Book == book1].Reviewer\n",
    "    book_2_reviewers = train[train.Book == book2].Reviewer\n",
    "    common_reviewers = set(book_1_reviewers).intersection(book_2_reviewers)\n",
    "\n",
    "    # Then we look for the reviews given by common reviewers\n",
    "    book_1_reviews = get_book_reviews(book1, common_reviewers)\n",
    "    book_2_reviews = get_book_reviews(book2, common_reviewers)\n",
    "    \n",
    "    # Calculate the Pearson Correlation Score\n",
    "    return cosin_sim(book_1_reviews.Rating, book_2_reviews.Rating)\n",
    "\n",
    "# Print the correlation score\n",
    "calculate_cosine_similarity(book_1,book_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's expand the idea to a larger set of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For performance reasons we will select the top most reviewed books as the calculation of similarity between all book pairs will take too much time. This is the reason why these calculations are executed seperately from the online scoring mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As I want to avoid dealing with a huge sparse matrix, I will only select the top most reviewed books for our example \n",
    "\n",
    "most_reviewed_books = pd.DataFrame({'count' : train.groupby([\"Book\"]).size()})\\\n",
    "                                    .reset_index().sort_values(['count'],ascending = False)\n",
    "\n",
    "most_reviewed_books.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the list of the most reviewed books\n",
    "\n",
    "top_books = []\n",
    "\n",
    "for i in most_reviewed_books.Book[0:20]:\n",
    "    top_books.append(i)\n",
    "\n",
    "    \n",
    "# calculate the correlation for our top books\n",
    "correlation_coefficient = []\n",
    "\n",
    "for book1 in top_books:\n",
    "    print \"Calculating the correlations for:\", book1\n",
    "    for book2 in top_books:\n",
    "        if book1 != book2:\n",
    "            row = [book1, book2] + [calculate_pearson_correlation(book1, book2)]\n",
    "            correlation_coefficient.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a look at what the table of correlation looks like\n",
    "cols = [\"Book_1\", \"Book_2\", \"Correlation\"]\n",
    "correlation_coefficient = pd.DataFrame(correlation_coefficient, columns=cols).sort_values('Correlation')\n",
    "#correlation_coefficient.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's clean up some of the titles\n",
    "correlation_coefficient= correlation_coefficient.replace([\"Where the Heart Is (Oprah's Book Club (Paperback))\", \"Harry Potter and the Goblet of Fire (Book 4)\",\"The Red Tent (Bestselling Backlist)\", \"Harry Potter and the Chamber of Secrets (Book 2)\",\n",
    "                                 \"Angels &amp\", \"Harry Potter and the Sorcerer's Stone (Harry Potter (Paperback))\", \"Harry Potter and the Prisoner of Azkaban (Book 3)\"],\n",
    "                                [\"Where the Heart Is (Oprah's Book Club)\", \"Harry Potter and the Goblet of Fire\", \"The Red Tent\", \"Harry Potter and the Chamber of Secrets\", \"Angels & Demons\",\n",
    "                                 \"Harry Potter and the Sorcerer's Stone\", \"Harry Potter and the Prisoner of Azkaban\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Also update the top book list\n",
    "top_books[6] = \"Harry Potter and the Chamber of Secrets\"\n",
    "top_books[7] = \"Angels & Demons\"\n",
    "top_books[9] = \"Harry Potter and the Sorcerer's Stone\"\n",
    "top_books[10] = \"Harry Potter and the Prisoner of Azkaban\"\n",
    "top_books[12] = \"The Red Tent\"\n",
    "top_books[13] = \"Harry Potter and the Goblet of Fire\"\n",
    "top_books[18] = \"Where the Heart Is (Oprah's Book Club)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to get the correlation between two books\n",
    "def calc_correlation(corr, book1, book2):\n",
    "    mask = (corr.Book_1==book1) & (corr.Book_2==book2)\n",
    "    row = corr[mask]\n",
    "    corr = row\n",
    "    return corr.sum(axis=1).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_correlation(correlation_coefficient,\"Harry Potter and the Sorcerer's Stone\", \"Harry Potter and the Chamber of Secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check: Find similar books to \"Harry Potter and the Sorcerer's Stone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the sorted correlations for Harry Potter\n",
    "my_book = \"Harry Potter and the Sorcerer's Stone\"\n",
    "\n",
    "results = []\n",
    "for b in top_books:\n",
    "    if my_book!=b:\n",
    "        results.append((b, calc_correlation(correlation_coefficient, my_book, b)))\n",
    "sorted(results, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: create a function *predict* which takes a book as input and outputs a list with other books, sorted from most correlated to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(my_book):\n",
    "    results = []\n",
    "    for other_book in top_books:\n",
    "        if my_book != other_book:\n",
    "            correlation = calc_correlation(correlation_coefficient, my_book, other_book)\n",
    "            results.append((my_book, other_book, correlation)) \n",
    "    return pd.DataFrame.from_dict(sorted(results, key=lambda x: x[2], reverse=True))[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test for any given book, what suggestions we should consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(\"Harry Potter and the Sorcerer's Stone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: What is needed to apply the cosine-similarity function we build before, for this recommender?\n",
    "\n",
    "**Question 2**: How could we improve the Item-based recommender system?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**: How would you apply this in your company or job?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Part of the code is from: http://www.mickaellegal.com/blog/2014/1/30/how-to-build-a-recommender\n",
    "- CF technique backgrounds are explained at http://www.hindawi.com/journals/aai/2009/421425/\n",
    "- Item-Based Top-N Recommendation Algorithms (Deshpande and Karypis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
